{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upskills clustering\n",
    "\n",
    "A number of experiments on trying to make sense of the ~200 job offers compiled as part of the upskills project. \n",
    "The core idea is trying to organise the kib offers. Since we do not have any annotation, we have opted for carrying out a clustering. Both representations and clustering come from [scikit](https://scikit-learn.org/) (twiking a bit the tokenizer)\n",
    "\n",
    "**Representations**\n",
    "\n",
    "\n",
    "* [TF-IDF](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "* [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "* [SVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD) (LSA)\n",
    "\n",
    "I had considered doc2vec at first, but the computation was taking way too long\n",
    "\n",
    "**Clustering alternatives**\n",
    "* [Birch](https://scikit-learn.org/stable/modules/clustering.html#birch). Requires the number of clusters. \n",
    "* [Afifinity propagation](https://scikit-learn.org/stable/modules/clustering.html#affinity-propagation). Estimates a reasonable number of clusters.\n",
    "* [Meanshift](https://scikit-learn.org/stable/modules/clustering.html#mean-shift).  Estimates a reasonable number of clusters.\n",
    "\n",
    "**Requirements (non-python-standard)**\n",
    "* spacy 3.0. Tokenization, lemmatization\n",
    "* sklearn. Feature computation, clustering\n",
    "* pandas. Dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7 MB 1.3 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.8/site-packages (from en-core-web-sm==3.0.0) (3.0.3)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (54.0.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.20.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.25.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.58.0)\n",
      "Requirement already satisfied: pathy in /usr/local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.26.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.8/site-packages (from pathy->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Downloading the model for spacy \n",
    "# RUN ONLY IF YOU DON'T HAVE THE MODELS READY\n",
    "! python3 -m spacy download en_core_web_sm\n",
    "# ! python -V\n",
    "# ! which python\n",
    "# ! pip3 install --upgrade --upgrade-strategy eager sklearn\n",
    "# ! pip3 install --upgrade spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.3\n"
     ]
    }
   ],
   "source": [
    "# checking the version of spacy (don't run)\n",
    "import spacy as kk\n",
    "  \n",
    "# Check the version \n",
    "print(kk.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim.models as g\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import spacy \n",
    "\n",
    "# from spacy.tokenizer import Tokenizer\n",
    "# from spacy.lang.en import English\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "#inference hyper-parameters\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HERE ARE THE CONSTANTS THAT YOU MIGHT WANT TO MODIFY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the path to the corpus\n",
    "path = \"/Users/albarron/tmp/adriano/txt_raw\"\n",
    "\n",
    "# the number of PCA components (e.g., 2, 20, 100)\n",
    "PCA_COMPONENTS = 100\n",
    "SVD_COMPONENTS = 100\n",
    "# From when considering to use doc2vec. Not really necessary\n",
    "# model = \"/Users/albarron/corpora/embeddings/doc2vec/enwiki_dbow/doc2vec.bin\"\n",
    "#load model\n",
    "# m = g.Doc2Vec.load(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(path):\n",
    "    \"\"\"Loads all the txt files in the path folder and \n",
    "    returns them, with their full path\"\"\"\n",
    "    my_files = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        my_files.extend(os.path.join(path, file) for file in files if file.endswith(\".txt\"))\n",
    "    return my_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml(file):\n",
    "    \"\"\"Extract the contents from an xml file (which in this \n",
    "    corpus actually have txt extension) and returns it as \n",
    "    a dictionary. \n",
    "    The assumed tags are: 'id', 'jobtitle', 'about', 'jobdesc', \n",
    "    'keyinfo', 'benefits'.\n",
    "    If jobdesc contains internal tags, all their text is simply \n",
    "    merged.\n",
    "    \"\"\"\n",
    "    with open(file) as f:\n",
    "        elements = {}\n",
    "        tree = ET.fromstring(f.read())\n",
    "        elements['id'] = tree.attrib['id']\n",
    "\n",
    "        for child in tree: #ET.fromstring(f.read()):\n",
    "            # elements[child.tag] = child.text\n",
    "            elements[child.tag] = ' '.join(child.itertext()).lower()\n",
    "    return elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all the files into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       localization editor,\\njapanese\\nlanguages are...\n",
      "1       german; english; applied linguistics; computa...\n",
      "2       english; computational linguistics; general l...\n",
      "3        english; french; computational linguistics; ...\n",
      "4       english; computational linguistics: senior an...\n",
      "                             ...                        \n",
      "193     french canadian linguist - siri tts\\napple is...\n",
      "194      english; german; spanish; applied linguistic...\n",
      "195     french; german; syntax: analytical linguist, ...\n",
      "196     swedish; computational linguistics; lexicogra...\n",
      "197     senior social analyst, data\\noverview\\nthe id...\n",
      "Name: jobtitle_desc, Length: 198, dtype: object\n"
     ]
    }
   ],
   "source": [
    "files = find_files(path)\n",
    "df = pd.DataFrame(\n",
    "    columns=['id', 'jobtitle', 'about', 'jobdesc', 'keyinfo', 'benefits'])\n",
    "\n",
    "for file in files:\n",
    "#     print(file)\n",
    "    d = extract_xml(file)\n",
    "    df = df.append(d, ignore_index=True)\n",
    "\n",
    "# replace empty jobtitles (NAN in a dataframe) with ''\n",
    "df.jobtitle = df.jobtitle.fillna('')\n",
    "# Add a new column combining offer title and description\n",
    "df['jobtitle_desc'] = \" \" + df.jobtitle + '\\n' + df.jobdesc\n",
    "df.set_index('id')\n",
    "# print(df.jobdesc)\n",
    "print(df.jobtitle_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representations production\n",
    "\n",
    "### Alternative 1: tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'be', 'a', 'very', 'small', 'test', 'try', 'to', 'check', 'if', 'I', 'be', 'or', 'you', 'be', 'actually', 'get', 'lemmas', 'and', 'not', 'token']\n"
     ]
    }
   ],
   "source": [
    "# nlp = English()\n",
    "# tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "# nlp.add_pipe(\"lemmatizer\", config={\"mode\": \"lookup\"})\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# config = {\"mode\": \"rule\"}\n",
    "# nlp.add_pipe(\"lemmatizer\", config=config)\n",
    "# This usually happens under the hood\n",
    "# processed = nlp(doc)\n",
    "\n",
    "\n",
    "doc = nlp(\"this is a very small tests trying to check if I am or you are actually getting lemmas and not tokens\")\n",
    "# lemmas = set([w.lemma for w in doc])\n",
    "# tokens = set([w for w in doc])\n",
    "# print (len(lemmas), len(tokens))\n",
    "# print(doc)\n",
    "print([token.lemma_ for token in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ca', 'far', 'make', 'nt'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary: 2317\n",
      "Shape of the matrix: (198, 2317)\n"
     ]
    }
   ],
   "source": [
    "# tfidf_docs = pd.DataFrame(tfidf_docs)\n",
    "# # # centers the vectorized documents (BOW vectors) by subtracting the mean\n",
    "# # tfidf_docs = tfidf_docs - tfidf_docs.mean()\n",
    "\n",
    "def lemmatized_words(doc):\n",
    "    \"\"\"A tokenizer based on spacy to add lemmas to the vector, \n",
    "    rather than tokens. It also ignores sequences of spaces \n",
    "    and 1-character tokens\"\"\"\n",
    "    doc = nlp(doc)\n",
    "    return (w.lemma_ for w in doc if w.lemma_.strip() !=\"\" and len(w.lemma_) > 1)\n",
    "\n",
    "# Without lemmatization; the vocabulary is huge: 75.4k.\n",
    "# lemm_vectorizer = TfidfVectorizer(tokenizer=tokenizer) \n",
    "\n",
    "# With lemmatization; the vocabulary is reasonable < 3k\n",
    "lemm_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=lemmatized_words, # the tokenizer from the function \n",
    "    stop_words='english', \n",
    "    min_df=2)\n",
    "\n",
    "tfidf_docs = lemm_vectorizer.fit_transform(raw_documents=df.jobtitle_desc).toarray()\n",
    "print(\"Size of the vocabulary:\", len(lemm_vectorizer.vocabulary_))\n",
    "\n",
    "tfidf_docs = pd.DataFrame(tfidf_docs)\n",
    "print(\"Shape of the matrix:\", tfidf_docs.shape)\n",
    "\n",
    "# Uncomment if you want to see the features (vocabulary)\n",
    "# print(\"Feature names:\\n\", lemm_vectorizer.get_feature_names())\n",
    "\n",
    "# print(tfidf_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative 2: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "      <th>...</th>\n",
       "      <th>topic90</th>\n",
       "      <th>topic91</th>\n",
       "      <th>topic92</th>\n",
       "      <th>topic93</th>\n",
       "      <th>topic94</th>\n",
       "      <th>topic95</th>\n",
       "      <th>topic96</th>\n",
       "      <th>topic97</th>\n",
       "      <th>topic98</th>\n",
       "      <th>topic99</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Facebook001</th>\n",
       "      <td>-0.739</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.921</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.767</td>\n",
       "      <td>-0.612</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linguist014</th>\n",
       "      <td>0.504</td>\n",
       "      <td>-0.772</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>-0.570</td>\n",
       "      <td>0.361</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.551</td>\n",
       "      <td>-0.382</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.501</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linguist028</th>\n",
       "      <td>0.826</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.449</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.978</td>\n",
       "      <td>-1.857</td>\n",
       "      <td>-1.199</td>\n",
       "      <td>-0.666</td>\n",
       "      <td>-1.143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.368</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>-0.156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linguist029</th>\n",
       "      <td>0.971</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>0.295</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>1.131</td>\n",
       "      <td>-1.231</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-1.063</td>\n",
       "      <td>-1.616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.452</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linguist015</th>\n",
       "      <td>-0.306</td>\n",
       "      <td>-0.476</td>\n",
       "      <td>-1.245</td>\n",
       "      <td>3.051</td>\n",
       "      <td>1.819</td>\n",
       "      <td>0.225</td>\n",
       "      <td>-0.562</td>\n",
       "      <td>-0.439</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>0.208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.262</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linguist001</th>\n",
       "      <td>1.071</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.806</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>1.113</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.447</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.450</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.401</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>0.227</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  \\\n",
       "id                                                                            \n",
       "Facebook001  -0.739  -0.140  -0.029  -0.298   0.086  -0.097  -0.921  -0.159   \n",
       "Linguist014   0.504  -0.772  -0.043  -0.135  -0.570   0.361  -0.030   0.464   \n",
       "Linguist028   0.826  -0.059   0.449  -0.100  -0.070   0.978  -1.857  -1.199   \n",
       "Linguist029   0.971  -0.068   0.295  -0.180  -0.228   1.131  -1.231  -0.983   \n",
       "Linguist015  -0.306  -0.476  -1.245   3.051   1.819   0.225  -0.562  -0.439   \n",
       "Linguist001   1.071  -0.269   0.009  -0.011  -0.400   0.806  -0.168   1.113   \n",
       "\n",
       "             topic8  topic9  ...  topic90  topic91  topic92  topic93  topic94  \\\n",
       "id                           ...                                                \n",
       "Facebook001   0.174   0.017  ...    0.663   -0.073    0.767   -0.612    0.396   \n",
       "Linguist014   0.551  -0.382  ...   -0.501   -0.159   -0.082    0.293    0.266   \n",
       "Linguist028  -0.666  -1.143  ...    0.032   -0.070   -0.191   -0.004    0.092   \n",
       "Linguist029  -1.063  -1.616  ...    0.302    0.911    0.032    0.452   -0.065   \n",
       "Linguist015  -0.274   0.208  ...   -0.542   -0.015   -0.234    0.227    0.262   \n",
       "Linguist001  -0.048   0.447  ...   -0.003    0.450   -0.062    0.401   -0.301   \n",
       "\n",
       "             topic95  topic96  topic97  topic98  topic99  \n",
       "id                                                        \n",
       "Facebook001    0.298    0.130   -0.419   -0.045   -0.238  \n",
       "Linguist014    0.241    0.037    0.089    0.129    0.098  \n",
       "Linguist028   -0.176   -0.368    0.123   -0.286   -0.156  \n",
       "Linguist029    0.039    0.653    0.371    0.186   -0.149  \n",
       "Linguist015   -0.034   -0.400    0.117    0.164    0.083  \n",
       "Linguist001   -0.158    0.227   -0.243    0.041   -0.125  \n",
       "\n",
       "[6 rows x 100 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaling the tfidf vectors in [0,1] (recommended for PCA)\n",
    "scaler = MinMaxScaler()\n",
    "tfidf_docs_rescaled = scaler.fit_transform(tfidf_docs)\n",
    "\n",
    "# Computing the PCS with PCA_COMPONENTS\n",
    "pca = PCA(svd_solver = 'full', n_components=PCA_COMPONENTS)\n",
    "pca = pca.fit(tfidf_docs_rescaled)\n",
    "pca_topic_vectors = pca.transform(tfidf_docs_rescaled)\n",
    "\n",
    "columns = ['topic{}'.format(i) for i in range(pca.n_components)]\n",
    "pca_topic_vectors = pd.DataFrame(pca_topic_vectors, columns=columns, index=df.id)\n",
    "pca_topic_vectors.round(3).head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative 3: TruncatedSVD (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "      <th>...</th>\n",
       "      <th>topic90</th>\n",
       "      <th>topic91</th>\n",
       "      <th>topic92</th>\n",
       "      <th>topic93</th>\n",
       "      <th>topic94</th>\n",
       "      <th>topic95</th>\n",
       "      <th>topic96</th>\n",
       "      <th>topic97</th>\n",
       "      <th>topic98</th>\n",
       "      <th>topic99</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Facebook001</th>\n",
       "      <td>1.612</td>\n",
       "      <td>-0.715</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.893</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.763</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>0.172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linguist014</th>\n",
       "      <td>1.695</td>\n",
       "      <td>0.583</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.785</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>0.447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linguist028</th>\n",
       "      <td>2.210</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.319</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>0.985</td>\n",
       "      <td>-1.768</td>\n",
       "      <td>-1.480</td>\n",
       "      <td>0.988</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>-0.048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linguist029</th>\n",
       "      <td>2.313</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.237</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.537</td>\n",
       "      <td>1.049</td>\n",
       "      <td>-1.182</td>\n",
       "      <td>-1.602</td>\n",
       "      <td>0.708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.640</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.574</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linguist015</th>\n",
       "      <td>2.483</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>-1.697</td>\n",
       "      <td>0.804</td>\n",
       "      <td>2.130</td>\n",
       "      <td>2.393</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.509</td>\n",
       "      <td>-0.457</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.377</td>\n",
       "      <td>0.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linguist001</th>\n",
       "      <td>2.057</td>\n",
       "      <td>1.055</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.626</td>\n",
       "      <td>0.638</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>0.771</td>\n",
       "      <td>-0.868</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.724</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.448</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  \\\n",
       "id                                                                            \n",
       "Facebook001   1.612  -0.715  -0.102   0.091  -0.315  -0.006  -0.043  -0.893   \n",
       "Linguist014   1.695   0.583  -0.379   0.441   0.012  -0.785   0.263  -0.061   \n",
       "Linguist028   2.210   0.785   0.336   0.319  -0.110  -0.283   0.985  -1.768   \n",
       "Linguist029   2.313   0.920   0.202   0.237  -0.141  -0.537   1.049  -1.182   \n",
       "Linguist015   2.483  -0.438  -1.697   0.804   2.130   2.393   0.430  -0.509   \n",
       "Linguist001   2.057   1.055  -0.118   0.187   0.079  -0.626   0.638  -0.269   \n",
       "\n",
       "             topic8  topic9  ...  topic90  topic91  topic92  topic93  topic94  \\\n",
       "id                           ...                                                \n",
       "Facebook001  -0.018   0.212  ...    0.059    0.763   -0.211   -0.308    0.196   \n",
       "Linguist014   0.691   0.208  ...   -0.100   -0.147    0.417    0.123   -0.003   \n",
       "Linguist028  -1.480   0.988  ...    0.265    0.191    0.240    0.040   -0.251   \n",
       "Linguist029  -1.602   0.708  ...    0.353   -0.250   -0.640    0.158    0.598   \n",
       "Linguist015  -0.457  -0.135  ...   -0.132   -0.233    0.214    0.009   -0.162   \n",
       "Linguist001   0.771  -0.868  ...   -0.202   -0.724   -0.269   -0.169   -0.073   \n",
       "\n",
       "             topic95  topic96  topic97  topic98  topic99  \n",
       "id                                                        \n",
       "Facebook001   -0.311    0.485    0.015   -0.280    0.172  \n",
       "Linguist014   -0.144   -0.188    0.109   -0.308    0.447  \n",
       "Linguist028    0.090    0.163   -0.119   -0.310   -0.048  \n",
       "Linguist029    0.410    0.514    0.574   -0.038   -0.608  \n",
       "Linguist015   -0.004   -0.121   -0.007   -0.377    0.283  \n",
       "Linguist001   -0.448    0.157    0.096    0.590    0.157  \n",
       "\n",
       "[6 rows x 100 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=SVD_COMPONENTS)   #, n_iter=100)\n",
    "scaler = MinMaxScaler()\n",
    "tfidf_docs_rescaled = scaler.fit_transform(tfidf_docs)\n",
    "\n",
    "# Decomposes TF-IDF vectors and transforms them into topic vectors\n",
    "svd_topic_vectors = svd.fit(tfidf_docs_rescaled)\n",
    "svd_topic_vectors = svd.transform(tfidf_docs_rescaled)\n",
    "\n",
    "# print(svd_topic_vectors.shape)\n",
    "columns = ['topic{}'.format(i) for i in range(svd.n_components)]\n",
    "svd_topic_vectors = pd.DataFrame(svd_topic_vectors, columns=columns, index=df.id)\n",
    "\n",
    "# Display the top-6 vectors\n",
    "svd_topic_vectors.round(3).head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Notice that running every alternative would add the column with the clusters to the df (and would be included in the saved tsv)\n",
    "\n",
    "## Alternative 1: Birch (which requires the number of clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def birch(data, k):\n",
    "    \"Produces a clustering with k clusters for the given data\"\n",
    "    brc = Birch(branching_factor=50, n_clusters=k, threshold=0.1, compute_labels=True)\n",
    "    brc.fit(data)\n",
    "\n",
    "    clusters = brc.predict(data)\n",
    "\n",
    "    labels = brc.labels_\n",
    "    return clusters\n",
    "\n",
    "    \n",
    "    # print (\"Clusters: \")\n",
    "#     print (clusters)\n",
    "# print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Birch with tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1, 21):\n",
    "    clusters = birch(tfidf_docs, k)\n",
    "    df[\".\".join([\"birch\", \"tfidf\", str(k)])] = clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Birch with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1, 21):\n",
    "    clusters = birch(pca_topic_vectors, k)\n",
    "    df[\".\".join([\"birch\", \"pca\", str(k)])] = clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Birch with SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1, 21):\n",
    "    clusters = birch(svd_topic_vectors, k)\n",
    "    df[\".\".join([\"birch\", \"svd\", str(k)])] = clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative 2: [AffinityPropagation](https://scikit-learn.org/stable/modules/clustering.html#affinity-propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def aff_propagation(X):\n",
    "    \"\"\"Compute clusters based on Affinity Propagation\"\"\"\n",
    "    af = AffinityPropagation(random_state=None)\n",
    "    af.fit(X)\n",
    "    cluster_centers_indices = af.cluster_centers_indices_\n",
    "    labels = af.labels_\n",
    "\n",
    "    n_clusters_ = len(cluster_centers_indices)\n",
    "\n",
    "    print('Estimated number of clusters: %d' % n_clusters_)\n",
    "    # print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "    # print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "    # print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "    # print(\"Adjusted Rand Index: %0.3f\"\n",
    "    #       % metrics.adjusted_rand_score(labels_true, labels))\n",
    "    # print(\"Adjusted Mutual Information: %0.3f\"\n",
    "    #       % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
    "    # print(\"Silhouette Coefficient: %0.3f\"\n",
    "    #       % metrics.silhouette_score(X, labels, metric='sqeuclidean'))\n",
    "    # print(\"Silhouette Coefficient: %0.3f\"\n",
    "    #       % metrics.silhouette_score(X, labels, metric='sqeuclidean'))\n",
    "    print(labels)\n",
    "    return labels\n",
    "#     print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AffinityPropagation with tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 40\n",
      "[ 0 13  1  5 19 26  0  3  7  6 11 27 39 13 26 26  9 31  3  2  9 31 26  3\n",
      "  7 19  3 26  4 17  3 27  5  7 15 14 31  0 33  6 35  4  7  5 31  7 10  7\n",
      " 10 10 25 37 30  0 39  6 35  8  9 10 11 26 10 12 13  9 35 14 14  4 15 34\n",
      " 15 16 17  3 39  0 11  5 10 16  6 16 31  4 23 26 27 12 31 18 31 27 38 16\n",
      " 21 19 25 24 20 16 32 32 23 36  4 21 30 21 18 31  1 22 27 14 23 37 10 31\n",
      " 24 25 36 31 18 30 38 26 31  7 34 38  1 27 22  2 31 37  7 34 28 31  7 25\n",
      " 17 29 13  9 31 28 39 30 30 20 31 34 33 32 23 10 33  8  3 29 26  3  7  2\n",
      "  7 10 21 19 10 17 19 25  4 34 35 27 26 34 26 10 28 36 19 26  5  3 27 38\n",
      " 36 37 26 24 38 39]\n"
     ]
    }
   ],
   "source": [
    "# centers the vectorized documents (BOW vectors) by subtracting the mean\n",
    "X = tfidf_docs\n",
    "X = X - X.mean()\n",
    "clusters = aff_propagation(X)\n",
    "df[\"affprop.tfidf\"] = clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AffinityPropagation with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 25\n",
      "[ 2  0 13  4  1  0  2 24  2  2  0  2  2 20 20  0  2  2  3  2  2  2  2  3\n",
      "  0  1  3  2  2  2 24  2  4  2  8  2 20  2  0  2  2  2  2  4 20  2  2 20\n",
      "  6  6  2 23  2  2  5  2 22 19 20  6  0 20  7  2  0 20 22  2  0  2  2 20\n",
      "  8  2  0 20  2  2 20  4  2  2  2 20  2  2 20 20  2  0  2 20 12  2 24  0\n",
      "  2 21 15 20  9  2 10  2 11 20  2  2 20  2 20 12 13 17  2  2 11 23 14  9\n",
      " 20 15  2 16  2  2 24 20  2  2  0 24  2  2 17  0  2 23  2  2 18  9  2  0\n",
      " 20 20  0  2  2 18  2  2  2  9  9  0 20 10  2  2  2 19 20 20  0 20  2  2\n",
      "  2 20  2 21  2  0 21  2  2 20 22  2  0  0 20  7  0 20  2  0  2 20  2 20\n",
      "  2 23 23 20 24  2]\n"
     ]
    }
   ],
   "source": [
    "clusters = aff_propagation(pca_topic_vectors)\n",
    "df[\"affprop.pca\"] = clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AffinityPropagation with SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 25\n",
      "[22 19  0  4  1  2  2 24  2  2  3  2  2 19 22 22  2  2  3  2 22 22  2  3\n",
      " 19  1  3  2  2 19 24  2  4  2  5  2 19 22  2  2 22  2  2  4 22  2  2 19\n",
      "  8  8  2 23 22  2  6 22 21  7 19  8 19 19  9 22 10 19 21  2  2 22  2 19\n",
      "  5  2 22 19 22  2 22  4  2 22  2 19  2  2 22 19  2 22  2 22 14  2 24 22\n",
      "  2 20 15 19 11  2 12  2 13 19 22  2 19  2 22 14  0 17  2 22 13 23 22 11\n",
      " 19 15  2 16  2  2 24 19 22  2 19 24  2  2 17 22 22 23  2  2 18 11 22 22\n",
      " 22 19 10 22  2 18 22  2  2 11 11 22 19 12  2 22  2  7 19 19 22  3  2  2\n",
      "  2  2 22 20  2 22 20  2  2 19 21 22 22 22 19  9 22 19  2 19  2 22 22 19\n",
      " 19 23 23 22 24  2]\n"
     ]
    }
   ],
   "source": [
    "clusters = aff_propagation(svd_topic_vectors)\n",
    "df[\"affprop.svd\"] = clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative 3: [Meanshift](https://scikit-learn.org/stable/modules/clustering.html#mean-shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_shift(X):\n",
    "    \"\"\"Compute clusters based on Meanshift\"\"\"\n",
    "    # The following bandwidth can be automatically detected using\n",
    "    bandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=500)\n",
    "\n",
    "    ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    ms.fit(X)\n",
    "    labels = ms.labels_\n",
    "    cluster_centers = ms.cluster_centers_\n",
    "    \n",
    "    labels_unique = np.unique(labels)\n",
    "    n_clusters_ = len(labels_unique)\n",
    "\n",
    "    print(\"number of estimated clusters : %d\" % n_clusters_)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeanShift with tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of estimated clusters : 1\n"
     ]
    }
   ],
   "source": [
    "X = tfidf_docs\n",
    "X = X - X.mean()\n",
    "clusters = mean_shift(X)\n",
    "df[\"meanshift.tfidf\"] = clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeanShift with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of estimated clusters : 1\n"
     ]
    }
   ],
   "source": [
    "clusters = mean_shift(pca_topic_vectors)\n",
    "df[\"meanshift.pca\"] = clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeanShift with SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 25\n",
      "[21 18  0  4  1  2  2 24  2  2  3  2  2 18 21 21  2  2  3  2 21 21  2  3\n",
      " 18  1  3  2  2 18 24  2  4  2  8  2 18 21  2  2 21  2  2  4 21  2  2 18\n",
      "  7  7  2 23 21  2  5 21 20  6 18  7 18 18 22 21 16 18 20  2  2 21  2 18\n",
      "  8  2 21 18 21  2 21  4  2 21  2 18  2  2 21 18  2 21  2 21  9  2 24 21\n",
      "  2 19 10 18 11  2 17  2 12 18 21  2 18  2 21  9  0 13  2 21 12 23 21 11\n",
      " 18 10  2 14  2  2 24 18 21  2 18 24  2  2 13 21 21 23  2  2 15 11 21 21\n",
      " 21 18 16 21  2 15 21  2  2 11 11 21 18 17  2 21  2  6 18 18 21  3  2  2\n",
      "  2  2 21 19  2 21 19  2  2 18 20 21 21 21 18 22 21 18  2 18  2 21 21 18\n",
      " 18 23 23 21 24  2]\n"
     ]
    }
   ],
   "source": [
    "clusters = aff_propagation(svd_topic_vectors)\n",
    "df[\"meanshift.svd\"] = clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id                                           jobtitle  \\\n",
      "0  Facebook001                     localization editor,\\njapanese   \n",
      "1  Linguist014  german; english; applied linguistics; computat...   \n",
      "2  Linguist028  english; computational linguistics; general li...   \n",
      "\n",
      "                                               about  \\\n",
      "0  about the facebook company\\nfacebook's mission...   \n",
      "1                                                NaN   \n",
      "2  we are an equal opportunity employer and value...   \n",
      "\n",
      "                                             jobdesc  \\\n",
      "0  languages are key to our mission of bringing t...   \n",
      "1  description:\\n\\nthis is an exciting opportunit...   \n",
      "2  description:\\n\\nappen is the world's leading i...   \n",
      "\n",
      "                                             keyinfo benefits  \\\n",
      "0                                                         NaN   \n",
      "1  university or organization: nuance communicati...      NaN   \n",
      "2  university or organization: appen\\ndepartment:...      NaN   \n",
      "\n",
      "                                       jobtitle_desc  birch.tfidf.1  \\\n",
      "0   localization editor,\\njapanese\\nlanguages are...              0   \n",
      "1   german; english; applied linguistics; computa...              0   \n",
      "2   english; computational linguistics; general l...              0   \n",
      "\n",
      "   birch.tfidf.2  birch.tfidf.3  ...  birch.svd.17  birch.svd.18  \\\n",
      "0              0              0  ...             2             0   \n",
      "1              1              1  ...             5             2   \n",
      "2              1              1  ...            11            11   \n",
      "\n",
      "   birch.svd.19  birch.svd.20  affprop.tfidf  affprop.pca  affprop.svd  \\\n",
      "0             1             1              0            2           22   \n",
      "1             0             5             13            0           19   \n",
      "2            11            11              1           13            0   \n",
      "\n",
      "   meanshift.tfidf  meanshift.pca  meanshift.svd  \n",
      "0                0              0             21  \n",
      "1                0              0             18  \n",
      "2                0              0              0  \n",
      "\n",
      "[3 rows x 73 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head(3))\n",
    "# df.to_csv(\"/\".join([path, \"upskills_clusters.tsv\"]), sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
