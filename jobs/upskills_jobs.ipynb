{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upskills clustering\n",
    "\n",
    "A number of experiments on trying to make sense of the ~200 job offers compiled as part of the upskills project. \n",
    "The core idea is trying to organise the kib offers. Since we do not have any annotation, we have opted for carrying out a clustering. Both representations and clustering come from [scikit](https://scikit-learn.org/) (twiking a bit the tokenizer)\n",
    "\n",
    "**Representations**\n",
    "\n",
    "\n",
    "* [TF-IDF](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "* [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "* [SVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD) (LSA)\n",
    "\n",
    "I had considered doc2vec at first, but the computation was taking way too long\n",
    "\n",
    "**Clustering alternatives**\n",
    "* [Birch](https://scikit-learn.org/stable/modules/clustering.html#birch). Requires the number of clusters. \n",
    "* [Afifinity propagation](https://scikit-learn.org/stable/modules/clustering.html#affinity-propagation). Estimates the right number of clusters.\n",
    "* [Meanshift](https://scikit-learn.org/stable/modules/clustering.html#mean-shift).  Estimates the right number of clusters.\n",
    "\n",
    "**Requirements (non-python-standard)**\n",
    "* spacy 3.0. Tokenization, lemmatization\n",
    "* sklearn. Feature computation, clustering\n",
    "* pandas. Dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the model for spacy \n",
    "# RUN ONLY IF YOU DON'T HAVE THE MODELS READY\n",
    "! python3 -m spacy download en_core_web_sm\n",
    "# ! python -V\n",
    "# ! which python\n",
    "# ! pip3 install --upgrade --upgrade-strategy eager sklearn\n",
    "# ! pip3 install --upgrade spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.3\n"
     ]
    }
   ],
   "source": [
    "# checking the version of spacy (don't run)\n",
    "import spacy as kk\n",
    "  \n",
    "# Check the version \n",
    "print(kk.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim.models as g\n",
    "import os\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import spacy \n",
    "\n",
    "# from spacy.tokenizer import Tokenizer\n",
    "# from spacy.lang.en import English\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "#inference hyper-parameters\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HERE ARE THE CONSTANTS THAT YOU MIGHT WANT TO MODIFY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the path to the corpus\n",
    "path = \"/Users/albarron/tmp/adriano/txt_raw\"\n",
    "\n",
    "# the number of PCA components (e.g., 2, 20, 100)\n",
    "PCA_COMPONENTS = 20\n",
    "# From when considering to use doc2vec. Not really necessary\n",
    "# model = \"/Users/albarron/corpora/embeddings/doc2vec/enwiki_dbow/doc2vec.bin\"\n",
    "#load model\n",
    "# m = g.Doc2Vec.load(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(path):\n",
    "    \"\"\"Loads all the txt files in the path folder and \n",
    "    returns them, with their full path\"\"\"\n",
    "    my_files = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        my_files.extend(os.path.join(path, file) for file in files if file.endswith(\".txt\"))\n",
    "    return my_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml(file):\n",
    "    \"\"\"Extract the contents from an xml file (which in this \n",
    "    corpus actually have txt extension) and returns it as \n",
    "    a dictionary. \n",
    "    The assumed tags are: 'id', 'jobtitle', 'about', 'jobdesc', \n",
    "    'keyinfo', 'benefits'.\n",
    "    If jobdesc contains internal tags, all their text is simply \n",
    "    merged.\n",
    "    \"\"\"\n",
    "    with open(file) as f:\n",
    "        elements = {}\n",
    "        tree = ET.fromstring(f.read())\n",
    "        elements['id'] = tree.attrib['id']\n",
    "\n",
    "        for child in tree: #ET.fromstring(f.read()):\n",
    "            # elements[child.tag] = child.text\n",
    "            elements[child.tag] = ' '.join(child.itertext()).lower()\n",
    "    return elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all the files into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       localization editor,\\njapanese\\nlanguages are...\n",
      "1       german; english; applied linguistics; computa...\n",
      "2       english; computational linguistics; general l...\n",
      "3        english; french; computational linguistics; ...\n",
      "4       english; computational linguistics: senior an...\n",
      "                             ...                        \n",
      "193     french canadian linguist - siri tts\\napple is...\n",
      "194      english; german; spanish; applied linguistic...\n",
      "195     french; german; syntax: analytical linguist, ...\n",
      "196     swedish; computational linguistics; lexicogra...\n",
      "197     senior social analyst, data\\noverview\\nthe id...\n",
      "Name: jobtitle_desc, Length: 198, dtype: object\n"
     ]
    }
   ],
   "source": [
    "files = find_files(path)\n",
    "df = pd.DataFrame(\n",
    "    columns=['id', 'jobtitle', 'about', 'jobdesc', 'keyinfo', 'benefits'])\n",
    "\n",
    "for file in files:\n",
    "#     print(file)\n",
    "    d = extract_xml(file)\n",
    "    df = df.append(d, ignore_index=True)\n",
    "\n",
    "# replace empty jobtitles (NAN in a dataframe) with ''\n",
    "df.jobtitle = df.jobtitle.fillna('')\n",
    "# Add a new column combining offer title and description\n",
    "df['jobtitle_desc'] = \" \" + df.jobtitle + '\\n' + df.jobdesc\n",
    "df.set_index('id')\n",
    "# print(df.jobdesc)\n",
    "print(df.jobtitle_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representations production\n",
    "\n",
    "### Alternative 1: tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'be', 'a', 'very', 'small', 'test', 'try', 'to', 'check', 'if', 'I', 'be', 'or', 'you', 'be', 'actually', 'get', 'lemmas', 'and', 'not', 'token']\n"
     ]
    }
   ],
   "source": [
    "# nlp = English()\n",
    "# tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "# nlp.add_pipe(\"lemmatizer\", config={\"mode\": \"lookup\"})\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# config = {\"mode\": \"rule\"}\n",
    "# nlp.add_pipe(\"lemmatizer\", config=config)\n",
    "# This usually happens under the hood\n",
    "# processed = nlp(doc)\n",
    "\n",
    "\n",
    "doc = nlp(\"this is a very small tests trying to check if I am or you are actually getting lemmas and not tokens\")\n",
    "# lemmas = set([w.lemma for w in doc])\n",
    "# tokens = set([w for w in doc])\n",
    "# print (len(lemmas), len(tokens))\n",
    "# print(doc)\n",
    "print([token.lemma_ for token in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary: 2317\n",
      "Shape of the matrix: (198, 2317)\n"
     ]
    }
   ],
   "source": [
    "# tfidf_docs = pd.DataFrame(tfidf_docs)\n",
    "# # # centers the vectorized documents (BOW vectors) by subtracting the mean\n",
    "# # tfidf_docs = tfidf_docs - tfidf_docs.mean()\n",
    "\n",
    "def lemmatized_words(doc):\n",
    "    \"\"\"A tokenizer based on spacy to add lemmas to the vector, \n",
    "    rather than tokens. It also ignores sequences of spaces \n",
    "    and 1-character tokens\"\"\"\n",
    "    doc = nlp(doc)\n",
    "    return (w.lemma_ for w in doc if w.lemma_.strip() !=\"\" and len(w.lemma_) > 1)\n",
    "\n",
    "# Without lemmatization; the vocabulary is huge: 75.4k.\n",
    "# lemm_vectorizer = TfidfVectorizer(tokenizer=tokenizer) \n",
    "\n",
    "# With lemmatization; the vocabulary is reasonable < 3k\n",
    "lemm_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=lemmatized_words, # the tokenizer from the function \n",
    "    stop_words='english', \n",
    "    min_df=2)\n",
    "\n",
    "tfidf_docs = lemm_vectorizer.fit_transform(raw_documents=df.jobtitle_desc).toarray()\n",
    "print(\"Size of the vocabulary:\", len(lemm_vectorizer.vocabulary_))\n",
    "\n",
    "tfidf_docs = pd.DataFrame(tfidf_docs)\n",
    "print(\"Shape of the matrix:\", tfidf_docs.shape)\n",
    "\n",
    "# Uncomment if you want to see the features (vocabulary)\n",
    "# print(\"Feature names:\\n\", lemm_vectorizer.get_feature_names())\n",
    "\n",
    "# print(tfidf_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative 2: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "      <th>topic10</th>\n",
       "      <th>topic11</th>\n",
       "      <th>topic12</th>\n",
       "      <th>topic13</th>\n",
       "      <th>topic14</th>\n",
       "      <th>topic15</th>\n",
       "      <th>topic16</th>\n",
       "      <th>topic17</th>\n",
       "      <th>topic18</th>\n",
       "      <th>topic19</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Facebook001</th>\n",
       "      <td>-0.739</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.921</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.546</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.828</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.412</td>\n",
       "      <td>-1.293</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>0.402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linguist014</th>\n",
       "      <td>0.504</td>\n",
       "      <td>-0.772</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>-0.570</td>\n",
       "      <td>0.361</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.551</td>\n",
       "      <td>-0.382</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linguist028</th>\n",
       "      <td>0.826</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.449</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.978</td>\n",
       "      <td>-1.857</td>\n",
       "      <td>-1.199</td>\n",
       "      <td>-0.666</td>\n",
       "      <td>-1.143</td>\n",
       "      <td>-0.894</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-1.106</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.508</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>-0.248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linguist029</th>\n",
       "      <td>0.971</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>0.295</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>1.131</td>\n",
       "      <td>-1.231</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-1.063</td>\n",
       "      <td>-1.616</td>\n",
       "      <td>-0.769</td>\n",
       "      <td>1.005</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.420</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>0.232</td>\n",
       "      <td>-0.771</td>\n",
       "      <td>-0.917</td>\n",
       "      <td>0.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linguist015</th>\n",
       "      <td>-0.306</td>\n",
       "      <td>-0.476</td>\n",
       "      <td>-1.245</td>\n",
       "      <td>3.051</td>\n",
       "      <td>1.819</td>\n",
       "      <td>0.225</td>\n",
       "      <td>-0.562</td>\n",
       "      <td>-0.439</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.427</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.318</td>\n",
       "      <td>-0.481</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>0.432</td>\n",
       "      <td>-0.659</td>\n",
       "      <td>0.146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linguist001</th>\n",
       "      <td>1.071</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.806</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>1.113</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.447</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.581</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>0.469</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  \\\n",
       "id                                                                            \n",
       "Facebook001  -0.739  -0.140  -0.029  -0.298   0.086  -0.097  -0.921  -0.159   \n",
       "Linguist014   0.504  -0.772  -0.043  -0.135  -0.570   0.361  -0.030   0.464   \n",
       "Linguist028   0.826  -0.059   0.449  -0.100  -0.070   0.978  -1.857  -1.199   \n",
       "Linguist029   0.971  -0.068   0.295  -0.180  -0.228   1.131  -1.231  -0.983   \n",
       "Linguist015  -0.306  -0.476  -1.245   3.051   1.819   0.225  -0.562  -0.439   \n",
       "Linguist001   1.071  -0.269   0.009  -0.011  -0.400   0.806  -0.168   1.113   \n",
       "\n",
       "             topic8  topic9  topic10  topic11  topic12  topic13  topic14  \\\n",
       "id                                                                         \n",
       "Facebook001   0.174   0.017   -0.067   -0.546    0.119   -0.828   -0.267   \n",
       "Linguist014   0.551  -0.382    0.327    0.045    0.111   -0.290    0.267   \n",
       "Linguist028  -0.666  -1.143   -0.894    0.558    0.544   -0.075    0.038   \n",
       "Linguist029  -1.063  -1.616   -0.769    1.005    0.614    0.739    0.420   \n",
       "Linguist015  -0.274   0.208    0.195   -0.427    0.280    0.318   -0.481   \n",
       "Linguist001  -0.048   0.447   -0.069   -0.581   -0.316    0.469   -0.277   \n",
       "\n",
       "             topic15  topic16  topic17  topic18  topic19  \n",
       "id                                                        \n",
       "Facebook001   -0.059    0.412   -1.293   -0.218    0.402  \n",
       "Linguist014    0.000   -0.312   -0.156    0.162    0.300  \n",
       "Linguist028   -1.106    0.412    0.508   -0.277   -0.248  \n",
       "Linguist029   -0.217    0.232   -0.771   -0.917    0.497  \n",
       "Linguist015   -0.313   -0.330    0.432   -0.659    0.146  \n",
       "Linguist001   -0.262   -0.337    0.051   -0.361    0.289  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaling the tfidf vectors in [0,1] (recommended for PCA)\n",
    "scaler = MinMaxScaler()\n",
    "tfidf_docs_rescaled = scaler.fit_transform(tfidf_docs)\n",
    "\n",
    "# Computing the PCS with PCA_COMPONENTS\n",
    "pca = PCA(svd_solver = 'full', n_components=PCA_COMPONENTS)\n",
    "pca = pca.fit(tfidf_docs_rescaled)\n",
    "pca_topic_vectors = pca.transform(tfidf_docs_rescaled)\n",
    "\n",
    "columns = ['topic{}'.format(i) for i in range(pca.n_components)]\n",
    "pca_topic_vectors = pd.DataFrame(pca_topic_vectors, columns=columns, index=df.id)\n",
    "pca_topic_vectors.round(3).head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative 3: TruncatedSVD (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "      <th>...</th>\n",
       "      <th>topic90</th>\n",
       "      <th>topic91</th>\n",
       "      <th>topic92</th>\n",
       "      <th>topic93</th>\n",
       "      <th>topic94</th>\n",
       "      <th>topic95</th>\n",
       "      <th>topic96</th>\n",
       "      <th>topic97</th>\n",
       "      <th>topic98</th>\n",
       "      <th>topic99</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Facebook001</th>\n",
       "      <td>1.612</td>\n",
       "      <td>-0.715</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.893</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.527</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.606</td>\n",
       "      <td>-0.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linguist014</th>\n",
       "      <td>1.695</td>\n",
       "      <td>0.583</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.785</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.466</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linguist028</th>\n",
       "      <td>2.210</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.319</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>0.985</td>\n",
       "      <td>-1.767</td>\n",
       "      <td>-1.478</td>\n",
       "      <td>0.987</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.249</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.257</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linguist029</th>\n",
       "      <td>2.313</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.237</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.537</td>\n",
       "      <td>1.048</td>\n",
       "      <td>-1.181</td>\n",
       "      <td>-1.603</td>\n",
       "      <td>0.708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.346</td>\n",
       "      <td>-0.624</td>\n",
       "      <td>-0.424</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.183</td>\n",
       "      <td>-0.475</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>-0.344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linguist015</th>\n",
       "      <td>2.483</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>-1.697</td>\n",
       "      <td>0.804</td>\n",
       "      <td>2.130</td>\n",
       "      <td>2.393</td>\n",
       "      <td>0.431</td>\n",
       "      <td>-0.510</td>\n",
       "      <td>-0.459</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.498</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>-0.297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linguist001</th>\n",
       "      <td>2.057</td>\n",
       "      <td>1.055</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.626</td>\n",
       "      <td>0.639</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>0.770</td>\n",
       "      <td>-0.868</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.401</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.620</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  \\\n",
       "id                                                                            \n",
       "Facebook001   1.612  -0.715  -0.102   0.091  -0.315  -0.006  -0.043  -0.893   \n",
       "Linguist014   1.695   0.583  -0.379   0.441   0.013  -0.785   0.263  -0.062   \n",
       "Linguist028   2.210   0.785   0.336   0.319  -0.110  -0.283   0.985  -1.767   \n",
       "Linguist029   2.313   0.920   0.202   0.237  -0.141  -0.537   1.048  -1.181   \n",
       "Linguist015   2.483  -0.438  -1.697   0.804   2.130   2.393   0.431  -0.510   \n",
       "Linguist001   2.057   1.055  -0.118   0.187   0.079  -0.626   0.639  -0.269   \n",
       "\n",
       "             topic8  topic9  ...  topic90  topic91  topic92  topic93  topic94  \\\n",
       "id                           ...                                                \n",
       "Facebook001  -0.017   0.211  ...    0.553    0.300    0.303    0.019   -0.160   \n",
       "Linguist014   0.690   0.207  ...    0.240   -0.325    0.055    0.003    0.611   \n",
       "Linguist028  -1.478   0.987  ...   -0.249   -0.007    0.025    0.328    0.257   \n",
       "Linguist029  -1.603   0.708  ...    0.200    0.163    0.346   -0.624   -0.424   \n",
       "Linguist015  -0.459  -0.134  ...   -0.067    0.003    0.305    0.033    0.274   \n",
       "Linguist001   0.770  -0.868  ...   -0.021    0.401   -0.185   -0.620    0.024   \n",
       "\n",
       "             topic95  topic96  topic97  topic98  topic99  \n",
       "id                                                        \n",
       "Facebook001    0.015   -0.527    0.116    0.606   -0.120  \n",
       "Linguist014    0.466   -0.274    0.216    0.078   -0.342  \n",
       "Linguist028   -0.132    0.129   -0.343    0.039    0.228  \n",
       "Linguist029    0.021    0.183   -0.475   -0.255   -0.344  \n",
       "Linguist015    0.475    0.029   -0.498   -0.222   -0.297  \n",
       "Linguist001   -0.429   -0.010    0.120   -0.121   -0.172  \n",
       "\n",
       "[6 rows x 100 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=100)   #, n_iter=100)\n",
    "scaler = MinMaxScaler()\n",
    "tfidf_docs_rescaled = scaler.fit_transform(tfidf_docs)\n",
    "\n",
    "# Decomposes TF-IDF vectors and transforms them into topic vectors\n",
    "svd_topic_vectors = svd.fit(tfidf_docs_rescaled)\n",
    "svd_topic_vectors = svd.transform(tfidf_docs_rescaled)\n",
    "\n",
    "# print(svd_topic_vectors.shape)\n",
    "columns = ['topic{}'.format(i) for i in range(svd.n_components)]\n",
    "svd_topic_vectors = pd.DataFrame(svd_topic_vectors, columns=columns, index=df.id)\n",
    "\n",
    "# Display the top-6 vectors\n",
    "svd_topic_vectors.round(3).head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Notice that running every alternative would add the column with the clusters to the df (and would be included in the saved tsv)\n",
    "\n",
    "## Alternative 1: Birch (which requires the number of clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def birch(data, k):\n",
    "    \"Produces a clustering with k clusters for the given data\"\n",
    "    brc = Birch(branching_factor=50, n_clusters=k, threshold=0.1, compute_labels=True)\n",
    "    brc.fit(data)\n",
    "\n",
    "    clusters = brc.predict(data)\n",
    "\n",
    "    labels = brc.labels_\n",
    "    return clusters\n",
    "\n",
    "    \n",
    "    # print (\"Clusters: \")\n",
    "#     print (clusters)\n",
    "# print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Birch with tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1, 21):\n",
    "    clusters = birch(tfidf_docs, k)\n",
    "    df[\".\".join([\"birch\", \"tfidf\", str(k)])] = clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Birch with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1, 21):\n",
    "    clusters = birch(pca_topic_vectors, k)\n",
    "    df[\".\".join([\"birch\", \"pca\", str(k)])] = clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Birch with SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1, 21):\n",
    "    clusters = birch(svd_topic_vectors, k)\n",
    "    df[\".\".join([\"birch\", \"svd\", str(k)])] = clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id                                           jobtitle  \\\n",
      "0  Facebook001                     localization editor,\\njapanese   \n",
      "1  Linguist014  german; english; applied linguistics; computat...   \n",
      "2  Linguist028  english; computational linguistics; general li...   \n",
      "\n",
      "                                               about  \\\n",
      "0  about the facebook company\\nfacebook's mission...   \n",
      "1                                                NaN   \n",
      "2  we are an equal opportunity employer and value...   \n",
      "\n",
      "                                             jobdesc  \\\n",
      "0  languages are key to our mission of bringing t...   \n",
      "1  description:\\n\\nthis is an exciting opportunit...   \n",
      "2  description:\\n\\nappen is the world's leading i...   \n",
      "\n",
      "                                             keyinfo benefits  \\\n",
      "0                                                         NaN   \n",
      "1  university or organization: nuance communicati...      NaN   \n",
      "2  university or organization: appen\\ndepartment:...      NaN   \n",
      "\n",
      "                                       jobtitle_desc  birch.tfidf.1  \\\n",
      "0   localization editor,\\njapanese\\nlanguages are...              0   \n",
      "1   german; english; applied linguistics; computa...              0   \n",
      "2   english; computational linguistics; general l...              0   \n",
      "\n",
      "   birch.tfidf.2  birch.tfidf.3  ...  birch.svd.11  birch.svd.12  \\\n",
      "0              0              0  ...             2             0   \n",
      "1              1              1  ...             2             0   \n",
      "2              1              1  ...             6             6   \n",
      "\n",
      "   birch.svd.13  birch.svd.14  birch.svd.15  birch.svd.16  birch.svd.17  \\\n",
      "0             0             0             6             6             6   \n",
      "1             0             0             0             0             0   \n",
      "2             6             6            14            14            14   \n",
      "\n",
      "   birch.svd.18  birch.svd.19  birch.svd.20  \n",
      "0             6             6             2  \n",
      "1             2             0             9  \n",
      "2            14            14            14  \n",
      "\n",
      "[3 rows x 67 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head(3))\n",
    "# df.to_csv(\"/\".join([path, \"upskills_clusters.tsv\"]), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative 2: [AffinityPropagation](https://scikit-learn.org/stable/modules/clustering.html#affinity-propagation) with tf-idf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Compute Affinity Propagation\n",
    "X = tfidf_docs\n",
    "# centers the vectorized documents (BOW vectors) by subtracting the mean\n",
    "X = X - X.mean()\n",
    "\n",
    "af = AffinityPropagation(random_state=None)\n",
    "af.fit(X)\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "labels = af.labels_\n",
    "\n",
    "n_clusters_ = len(cluster_centers_indices)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "# print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "# print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "# print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "# print(\"Adjusted Rand Index: %0.3f\"\n",
    "#       % metrics.adjusted_rand_score(labels_true, labels))\n",
    "# print(\"Adjusted Mutual Information: %0.3f\"\n",
    "#       % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
    "# print(\"Silhouette Coefficient: %0.3f\"\n",
    "#       % metrics.silhouette_score(X, labels, metric='sqeuclidean'))\n",
    "# print(\"Silhouette Coefficient: %0.3f\"\n",
    "#       % metrics.silhouette_score(X, labels, metric='sqeuclidean'))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AffinityPropagation with tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying with [AffinityPropagation](https://scikit-learn.org/stable/modules/clustering.html#affinity-propagation) with PCA vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Compute Affinity Propagation\n",
    "X = pca_topic_vectors\n",
    "af = AffinityPropagation()\n",
    "af.fit(X)\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "labels = af.labels_\n",
    "\n",
    "n_clusters_ = len(cluster_centers_indices)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "# print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "# print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "# print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "# print(\"Adjusted Rand Index: %0.3f\"\n",
    "#       % metrics.adjusted_rand_score(labels_true, labels))\n",
    "# print(\"Adjusted Mutual Information: %0.3f\"\n",
    "#       % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
    "# print(\"Silhouette Coefficient: %0.3f\"\n",
    "#       % metrics.silhouette_score(X, labels, metric='sqeuclidean'))\n",
    "# print(\"Silhouette Coefficient: %0.3f\"\n",
    "#       % metrics.silhouette_score(X, labels, metric='sqeuclidean'))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying with [AffinityPropagation](https://scikit-learn.org/stable/modules/clustering.html#affinity-propagation) with SVD (LSA) vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Compute Affinity Propagation\n",
    "X = svd_topic_vectors\n",
    "af = AffinityPropagation()\n",
    "af.fit(X)\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "labels = af.labels_\n",
    "\n",
    "n_clusters_ = len(cluster_centers_indices)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "# print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "# print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "# print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "# print(\"Adjusted Rand Index: %0.3f\"\n",
    "#       % metrics.adjusted_rand_score(labels_true, labels))\n",
    "# print(\"Adjusted Mutual Information: %0.3f\"\n",
    "#       % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
    "# print(\"Silhouette Coefficient: %0.3f\"\n",
    "#       % metrics.silhouette_score(X, labels, metric='sqeuclidean'))\n",
    "# print(\"Silhouette Coefficient: %0.3f\"\n",
    "#       % metrics.silhouette_score(X, labels, metric='sqeuclidean'))\n",
    "print(labels)\n",
    "\n",
    "for id, cluster in zip(df.id, labels):\n",
    "    print (id, cluster)\n",
    "# print(\"\\t\".join([id, cluster]) for id, cluster in zip (df.id, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with Meanshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " # Trying with doc2vec \n",
    " # The computation is way to slow and we might need to use the cluster\n",
    " \n",
    "# test_docs=\"data/test_docs.txt\"\n",
    " \n",
    "\n",
    "\n",
    "# load the texts into lists\n",
    "test_ids = []\n",
    "test_docs = []\n",
    "X = []\n",
    "for i, row in df.iterrows():\n",
    "  print(row['id'])\n",
    "  test_ids.append(row['id'])\n",
    "  X.append(\n",
    "      m.infer_vector(\n",
    "          row['jobdesc'].strip().split(),\n",
    "          # [x.strip().split() for x in row['jobdesc']], \n",
    "          alpha=start_alpha, steps=infer_epoch) )\n",
    "      \n",
    "\n",
    "print(X)\n",
    "\n",
    "\n",
    "# test_docs = [ x.strip().split() for x in codecs.open(test_docs, \"r\", \"utf-8\").readlines() ]\n",
    " \n",
    "# print (test_docs)\n",
    "# \"\"\"\n",
    "# [['the', 'cardigan', 'welsh', 'corgi'........\n",
    "# \"\"\"\n",
    " \n",
    "# X=[]\n",
    "# for d in test_docs:\n",
    "     \n",
    "#     X.append( m.infer_vector(d, alpha=start_alpha, steps=infer_epoch) )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "k=3\n",
    " \n",
    " \n",
    "brc = Birch(branching_factor=50, n_clusters=k, threshold=0.1, compute_labels=True)\n",
    "brc.fit(X)\n",
    " \n",
    "clusters = brc.predict(X)\n",
    " \n",
    "labels = brc.labels_\n",
    " \n",
    " \n",
    "print (\"Clusters: \")\n",
    "print (clusters)\n",
    " \n",
    " \n",
    "# silhouette_score = metrics.silhouette_score(X, labels, metric='euclidean')\n",
    " \n",
    "# print (\"Silhouette_score: \")\n",
    "# print (silhouette_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The community has created multiple libraries for pre-processing, which include options for tokenisation. One of the most popular ones is [NLTK](http://www.nltk.org). \n",
    "\n",
    "Before using it, you should install it. If using pip, you should do: \n",
    "\n",
    "\\$ pip install --user -U nltk\n",
    "\n",
    "\\$ pip install --user -U numpy\n",
    "\n",
    "\n",
    "An now we can import and use one of its tokenisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer # import one of the many tokenizers available\n",
    "tokenizer = TreebankWordTokenizer()             # invoke it \n",
    "tokens = tokenizer.tokenize(txt)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, see the difference between tokenising with split() and with NLTK's treebank tokeniser on a different sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Monticello wasn't designated as UNESCO World Heritage Site until 1987.\"\n",
    "tokens_split = sentence.split()\n",
    "tokens_tree = tokenizer.tokenize(sentence)\n",
    "\n",
    "print(\"OUTPUT USING split()\\t\\t\", tokens_split)\n",
    "print(\"OUTPUT USING TreebankWordTokenizer\\t\", tokens_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation\n",
    "\n",
    "### Casefolding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence  = sentence.lower()\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Once again, we can use a regular expression to do stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(phrase):\n",
    "    return ' '.join([re.findall('^(.*ss|.*?)(s)?$',\n",
    "         word)[0][0].strip(\"'\") for word in phrase.lower()\n",
    "         .split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"'houses' \\t\\t->\", stem('houses'))\n",
    "print(\"'Doctor House's calls' \\t->\", stem(\"Doctor House's calls\"))\n",
    "print(\"'stress' \\t\\t->\", stem(\"stress\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we would need to include many more expressions to deal with all cases and exceptions.\n",
    "\n",
    "Instead, once again we can rely on a library. Let's consider the **Porter stemmer**, available in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer # Import the stemmer\n",
    "stemmer = PorterStemmer()                  # invoke the stemmer\n",
    "\n",
    "# Notice that we are \"tokenising\" and stemming in one line\n",
    "x = ' '.join([stemmer.stem(w).strip(\"'\") for w in \"dish washer's washed dishes\".split()])\n",
    "print(x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatisation\n",
    "\n",
    "This is a more complex process, compared to stemming. Let us go straight to use a library.\n",
    "In this particular case we are going to use NLTK's WordNet lemmatiser. If it is the first time you use it (or you are in an ephemeral environment!), you should download it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer # importing the lemmatiser\n",
    "lemmatizer = WordNetLemmatizer()        # invoking it\n",
    "\n",
    "print(\"'better' alone \\t->\",lemmatizer.lemmatize(\"better\"))\n",
    "print(\"'better' including it's part of speech (adj) \\t->\",lemmatizer.lemmatize(\"better\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick overview on representations\n",
    "\n",
    "### Bag of Words (BoW)\n",
    "\n",
    "First, let us see a simple construction, using a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the age of 26. Thomas\"\"\"\n",
    "\n",
    "sentence_bow = {}\n",
    "for token in sentence.split():\n",
    "     sentence_bow[token] = 1\n",
    "sorted(sentence_bow.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option would be using **pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the corpus\n",
    "sentences = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\\n\"\"\"\n",
    "sentences += \"\"\"Construction was done mostly by local masons and carpenters.\\n\"\"\"\n",
    "sentences += \"He moved into the South Pavilion in 1770.\\n\"\n",
    "sentences += \"\"\"Turning Monticello into a neoclassical masterpiece was Jefferson's obsession.\"\"\"\n",
    "\n",
    "# Loading the tokens into a dictionary (notice that we asume that each line is a document)\n",
    "corpus = {}\n",
    "for i, sent in enumerate(sentences.split('\\n')):\n",
    "    corpus['sent{}'.format(i)] = dict((tok, 1) for tok in\n",
    "         sent.split())\n",
    "\n",
    "# Loading the dictionary contents into a pandas dataframe. \n",
    "df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T\n",
    "# SEE THE .T, which transposes the matrix for visualisation purposes.\n",
    "\n",
    "\n",
    "df[df.columns[:10]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot vectors\n",
    "\n",
    "This is our input sentence (and its vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sentence = \"Thomas Jefferson began building Monticello at the age of 26.\"\n",
    "token_sequence = str.split(sentence)\n",
    "vocab = sorted(set(token_sequence))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we produce the one-hot representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(token_sequence)\n",
    "vocab_size = len(vocab)\n",
    "onehot_vectors = np.zeros((num_tokens, vocab_size), int) # create the |tokens| x |vocabulary size| matrix of zeros \n",
    "for i, word in enumerate(token_sequence):\n",
    "   onehot_vectors[i, vocab.index(word)] = 1  # set one to right dimension to 1\n",
    "\n",
    "print(\"Vocabulary:\\t\", vocab)\n",
    "print(\"Sentence:\\t\", token_sequence)\n",
    "onehot_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us bring pandas into the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(onehot_vectors, columns=vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
